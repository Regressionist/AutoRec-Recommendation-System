{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240447, 14277)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('ratings.csv',names=['userID','movieID','rating','time'])\n",
    "df.drop('time',axis=1,inplace=True)\n",
    "users=[k for k,v in df['userID'].value_counts().iteritems() if v>2]\n",
    "movies=[k for k,v in df['movieID'].value_counts().iteritems() if v>10]\n",
    "df=df[(df['userID'].isin(users)) & (df['movieID'].isin(movies))]\n",
    "df=df.pivot(index='userID',columns='movieID',values='rating')\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_matrix=df.values\n",
    "#mask_matrix=mask.values\n",
    "train_df=df.iloc[0:220000]\n",
    "#val_matrix=df_matrix[120000:130000]\n",
    "test_df=df.iloc[230000:].reset_index(drop=True)\n",
    "#train_mask=mask_matrix[0:120000]\n",
    "val_df=df.iloc[220000:230000].reset_index(drop=True)\n",
    "#test_mask=mask_matrix[130000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autorec(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size):\n",
    "        super(Autorec, self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        self.encoder=nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.decoder=nn.Linear(self.hidden_size, self.input_size)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        #self.decoder.weight.data = self.encoder.weight.data.transpose(0,1)\n",
    "        #self.register_buffer('input', torch.zeros(input_size))\n",
    "        \n",
    "    def forward(self, input_ratings):\n",
    "        self.input=input_ratings\n",
    "        enc_out = self.encoder(input_ratings)\n",
    "        dec_out = 5*self.sigmoid(self.decoder(enc_out))\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch(input_ratings, mask, autorec, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    output_ratings=autorec(input_ratings.type(torch.cuda.FloatTensor))*mask.type(torch.cuda.FloatTensor)\n",
    "    loss=criterion(output_ratings,input_ratings.type(torch.cuda.FloatTensor))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return torch.sqrt(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(k,input_ratings, output_ratings):\n",
    "    prec=[]\n",
    "    for i in range(output_ratings.size(0)):\n",
    "        total=torch.nonzero(output_ratings[i]).size(0)\n",
    "        if(total>=k):\n",
    "            ratings_pred,idx_pred=torch.topk(output_ratings[i],k)\n",
    "            ratings_actual,idx_actual=torch.topk(input_ratings[i],k)\n",
    "            prec.append((np.intersect1d((idx_pred).cpu().numpy(), (idx_actual).cpu().numpy()).shape[0])/k)\n",
    "    return np.mean(prec)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(input_ratings, mask, autorec):\n",
    "    with torch.no_grad():\n",
    "        input_ratings=input_ratings.type(torch.cuda.FloatTensor)\n",
    "        output_ratings=autorec(input_ratings)*mask.type(torch.cuda.FloatTensor)\n",
    "        #loss=torch.sqrt(criterion(output_ratings,input_ratings.type(torch.cuda.FloatTensor)))\n",
    "        idx=torch.nonzero(mask)\n",
    "        loss=0\n",
    "        for i in idx:\n",
    "            loss+=((output_ratings[i[0]][i[1]]-input_ratings[i[0]][i[1]]).item())**2\n",
    "    return np.sqrt(loss/idx.size(0)), precision(10,input_ratings,output_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorec=Autorec(hidden_size=500,input_size=train_df.shape[1])\n",
    "optimizer=optim.Adam(autorec.parameters())\n",
    "criterion=nn.MSELoss()\n",
    "device=torch.device('cuda')\n",
    "autorec=autorec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask=val_df.copy()\n",
    "val_mask[~val_mask.isnull()] = 1  # not nan\n",
    "val_mask[val_mask.isnull()] = 0   # nan\n",
    "val_df[val_df.isnull()] = 0   # nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 | Step: 1/5 | Training Loss: 0.03767562657594681 | Validation Loss: 2.0063\n",
      "%---Saving the model---%\n",
      "Batch: 1 | Step: 2/5 | Training Loss: 9.961256448179483 | Validation Loss: 1.0078\n",
      "%---Saving the model---%\n",
      "Batch: 1 | Step: 3/5 | Training Loss: 7.971252323128283 | Validation Loss: 0.9092\n",
      "%---Saving the model---%\n",
      "Batch: 1 | Step: 4/5 | Training Loss: 7.234345636330545 | Validation Loss: 0.8588\n",
      "%---Saving the model---%\n",
      "Batch: 1 | Step: 5/5 | Training Loss: 6.810999747365713 | Validation Loss: 0.8155\n",
      "%---Saving the model---%\n",
      "Batch: 2 | Step: 1/5 | Training Loss: 0.011857983656227589 | Validation Loss: 0.794\n",
      "%---Saving the model---%\n",
      "Batch: 2 | Step: 2/5 | Training Loss: 5.100716646760702 | Validation Loss: 0.7799\n",
      "%---Saving the model---%\n",
      "Batch: 2 | Step: 3/5 | Training Loss: 4.8099432503804564 | Validation Loss: 0.7808\n",
      "Batch: 2 | Step: 4/5 | Training Loss: 4.8241234216839075 | Validation Loss: 0.7839\n",
      "Batch: 2 | Step: 5/5 | Training Loss: 4.899771083146334 | Validation Loss: 0.7951\n",
      "Batch: 3 | Step: 1/5 | Training Loss: 0.010508396662771702 | Validation Loss: 0.8018\n",
      "Batch: 3 | Step: 2/5 | Training Loss: 5.163807983510196 | Validation Loss: 0.8051\n",
      "Batch: 3 | Step: 3/5 | Training Loss: 5.2777675576508045 | Validation Loss: 0.8092\n",
      "Batch: 3 | Step: 4/5 | Training Loss: 5.3647660608403385 | Validation Loss: 0.8214\n",
      "Batch: 3 | Step: 5/5 | Training Loss: 5.357017668895423 | Validation Loss: 0.8302\n",
      "Batch: 4 | Step: 1/5 | Training Loss: 0.01193583570420742 | Validation Loss: 0.8368\n",
      "Batch: 4 | Step: 2/5 | Training Loss: 5.664160325191915 | Validation Loss: 0.8383\n",
      "Batch: 4 | Step: 3/5 | Training Loss: 5.762761862017214 | Validation Loss: 0.8385\n",
      "Batch: 4 | Step: 4/5 | Training Loss: 5.781239036470652 | Validation Loss: 0.851\n",
      "Batch: 4 | Step: 5/5 | Training Loss: 5.705698485486209 | Validation Loss: 0.8561\n",
      "Batch: 5 | Step: 1/5 | Training Loss: 0.011558790691196918 | Validation Loss: 0.8661\n",
      "Batch: 5 | Step: 2/5 | Training Loss: 5.899574939161539 | Validation Loss: 0.8604\n",
      "Batch: 5 | Step: 3/5 | Training Loss: 5.942847536876798 | Validation Loss: 0.8627\n",
      "Batch: 5 | Step: 4/5 | Training Loss: 5.968040248379111 | Validation Loss: 0.869\n",
      "Batch: 5 | Step: 5/5 | Training Loss: 5.878046809695661 | Validation Loss: 0.8734\n"
     ]
    }
   ],
   "source": [
    "num_batches=5\n",
    "val_benchmark=10\n",
    "\n",
    "input_users_val=torch.from_numpy(val_df.values).to(device).detach()\n",
    "mask_val=torch.from_numpy(val_mask.values).to(device).detach()\n",
    "\n",
    "for batch in range(0,num_batches):\n",
    "    running_loss=0\n",
    "    #train_df = shuffle(train_df)\n",
    "    for i in range(0,train_df.shape[0],100):\n",
    "        #print(i)\n",
    "        tdf=train_df.iloc[i:i+100].copy()\n",
    "        train_mask=tdf.copy()\n",
    "        train_mask[~train_mask.isnull()] = 1  # not nan\n",
    "        train_mask[train_mask.isnull()] = 0   # nan\n",
    "        tdf[tdf.isnull()] = 0\n",
    "        input_users=Variable(torch.from_numpy(tdf.values)).to(device)\n",
    "        input_mask=torch.from_numpy(train_mask.values).to(device)\n",
    "        loss=train_minibatch(input_users, input_mask, autorec, optimizer, criterion)\n",
    "        running_loss+=loss.item()\n",
    "        if (i)%44000==0:\n",
    "            val_loss=validation(input_users_val, mask_val, autorec)\n",
    "            print ('Batch: {} | Step: {}/{} | Training Loss: {} | Validation Loss: {}'.format(batch+1,int(i/44000)+1,5,running_loss,round(val_loss,4)))\n",
    "            running_loss=0\n",
    "            if(val_loss<val_benchmark):\n",
    "                print ('%---Saving the model---%')\n",
    "                torch.save({\n",
    "                    'step':i+1,\n",
    "                    'autorec_state_dict': autorec.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'batch':batch,\n",
    "                    'loss':val_loss\n",
    "                    },'model.pth')\n",
    "                val_benchmark=val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autorec(\n",
       "  (encoder): Linear(in_features=14277, out_features=500, bias=True)\n",
       "  (decoder): Linear(in_features=500, out_features=14277, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('model.pth')\n",
    "autorec.load_state_dict(checkpoint['autorec_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "autorec.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask=test_df.copy()\n",
    "test_mask[~test_mask.isnull()] = 1  # not nan\n",
    "test_mask[test_mask.isnull()] = 0   # nan\n",
    "test_df[test_df.isnull()] = 0   # nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_users_test=torch.from_numpy(test_df.values).to(device).detach()\n",
    "mask_test=torch.from_numpy(test_mask.values).to(device).detach()\n",
    "#print ('RMSE: {}, Precision@10: {}'.format(validation(input_users_test, mask_test, autorec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8058728485650766, Precision@10: 0.8329706202393907\n"
     ]
    }
   ],
   "source": [
    "rmse,prec=validation(input_users_test, mask_test, autorec)\n",
    "print ('RMSE: {}, Precision@10: {}'.format(rmse,prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
