{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240447, 14277)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('ratings.csv',names=['userID','movieID','rating','time'])\n",
    "df.drop('time',axis=1,inplace=True)\n",
    "users=[k for k,v in df['userID'].value_counts().iteritems() if v>2]\n",
    "movies=[k for k,v in df['movieID'].value_counts().iteritems() if v>10]\n",
    "df=df[(df['userID'].isin(users)) & (df['movieID'].isin(movies))]\n",
    "df=df.pivot(index='userID',columns='movieID',values='rating')\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_matrix=df.values\n",
    "#mask_matrix=mask.values\n",
    "train_df=df.iloc[0:220000]\n",
    "#val_matrix=df_matrix[120000:130000]\n",
    "test_df=df.iloc[230000:].reset_index(drop=True)\n",
    "#train_mask=mask_matrix[0:120000]\n",
    "val_df=df.iloc[220000:230000].reset_index(drop=True)\n",
    "#test_mask=mask_matrix[130000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autorec(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size):\n",
    "        super(Autorec, self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        self.encoder=nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.decoder=nn.Linear(self.hidden_size, self.input_size)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.decoder.weight.data = self.encoder.weight.data.transpose(0,1)\n",
    "        #self.register_buffer('input', torch.zeros(input_size))\n",
    "        \n",
    "    def forward(self, input_ratings):\n",
    "        self.input=input_ratings\n",
    "        enc_out = self.encoder(input_ratings)\n",
    "        dec_out = 5*self.sigmoid(self.decoder(enc_out))\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch(input_ratings, mask, autorec, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    output_ratings=autorec(input_ratings.type(torch.cuda.FloatTensor))*mask.type(torch.cuda.FloatTensor)\n",
    "    loss=criterion(output_ratings,input_ratings.type(torch.cuda.FloatTensor))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return torch.sqrt(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(input_ratings, mask, autorec):\n",
    "    with torch.no_grad():\n",
    "        input_ratings=input_ratings.type(torch.cuda.FloatTensor)\n",
    "        output_ratings=autorec(input_ratings)*mask.type(torch.cuda.FloatTensor)\n",
    "        #loss=torch.sqrt(criterion(output_ratings,input_ratings.type(torch.cuda.FloatTensor)))\n",
    "        idx=torch.nonzero(mask)\n",
    "        loss=0\n",
    "        for i in idx:\n",
    "            loss+=((output_ratings[i[0]][i[1]]-input_ratings[i[0]][i[1]]).item())**2\n",
    "    return np.sqrt(loss/idx.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorec=Autorec(hidden_size=500,input_size=train_df.shape[1])\n",
    "optimizer=optim.Adam(autorec.parameters())\n",
    "criterion=nn.MSELoss()\n",
    "device=torch.device('cuda')\n",
    "autorec=autorec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask=val_df.copy()\n",
    "val_mask[~val_mask.isnull()] = 1  # not nan\n",
    "val_mask[val_mask.isnull()] = 0   # nan\n",
    "val_df[val_df.isnull()] = 0   # nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches=5\n",
    "val_benchmark=10\n",
    "\n",
    "input_users_val=torch.from_numpy(val_df.values).to(device).detach()\n",
    "mask_val=torch.from_numpy(val_mask.values).to(device).detach()\n",
    "\n",
    "for batch in range(0,num_batches):\n",
    "    running_loss=0\n",
    "    #train_df = shuffle(train_df)\n",
    "    for i in range(0,train_df.shape[0],100):\n",
    "        #print(i)\n",
    "        tdf=train_df.iloc[i:i+100].copy()\n",
    "        train_mask=tdf.copy()\n",
    "        train_mask[~train_mask.isnull()] = 1  # not nan\n",
    "        train_mask[train_mask.isnull()] = 0   # nan\n",
    "        tdf[tdf.isnull()] = 0\n",
    "        input_users=Variable(torch.from_numpy(tdf.values)).to(device)\n",
    "        input_mask=torch.from_numpy(train_mask.values).to(device)\n",
    "        loss=train_minibatch(input_users, input_mask, autorec, optimizer, criterion)\n",
    "        running_loss+=loss.item()\n",
    "        if (i)%44000==0:\n",
    "            val_loss=validation(input_users_val, mask_val, autorec)\n",
    "            print ('Batch: {} | Step: {}/{} | Training Loss: {} | Validation Loss: {}'.format(batch+1,int(i/44000)+1,5,round(running_loss,4),round(val_loss,4)))\n",
    "            running_loss=0\n",
    "            if(val_loss<val_benchmark):\n",
    "                print ('%---Saving the model---%')\n",
    "                torch.save({\n",
    "                    'step':i+1,\n",
    "                    'autorec_state_dict': autorec.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'batch':batch,\n",
    "                    'loss':val_loss\n",
    "                    },'model.pth')\n",
    "                val_benchmark=val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Autorec:\n\tsize mismatch for input: copying a param of torch.Size([14277]) from checkpoint, where the shape is torch.Size([10000, 14277]) in current model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-eb80818fbc46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautorec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'autorec_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mautorec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Autorec:\n\tsize mismatch for input: copying a param of torch.Size([14277]) from checkpoint, where the shape is torch.Size([10000, 14277]) in current model."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('model.pth')\n",
    "autorec.load_state_dict(checkpoint['autorec_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "autorec.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask=test_df.copy()\n",
    "test_mask[~test_mask.isnull()] = 1  # not nan\n",
    "test_mask[test_mask.isnull()] = 0   # nan\n",
    "test_df[test_df.isnull()] = 0   # nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8465419929098972"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_users_val=torch.from_numpy(test_df.values).to(device).detach()\n",
    "mask_test=torch.from_numpy(test_mask.values).to(device).detach()\n",
    "validation(input_users_val, mask_test, autorec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0036, -0.0464,  0.0041,  ...,  0.0403, -0.0139,  0.0173],\n",
       "        [ 0.0017,  0.1003, -0.0075,  ..., -0.0125,  0.0245, -0.0524],\n",
       "        [-0.0517,  0.0076, -0.0221,  ...,  0.0615,  0.0768, -0.0070],\n",
       "        ...,\n",
       "        [-0.0299, -0.0065, -0.0376,  ...,  0.0551,  0.0400,  0.0846],\n",
       "        [ 0.0404, -0.0047, -0.0087,  ...,  0.0545,  0.0042, -0.0347],\n",
       "        [ 0.0006,  0.0204, -0.0200,  ...,  0.0152, -0.0204,  0.0281]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorec.encoder.weight.data.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0377, -0.0119,  0.0262,  ...,  0.0343, -0.0206, -0.0137],\n",
       "        [-0.0417,  0.0707, -0.0390,  ...,  0.0310,  0.0063,  0.0040],\n",
       "        [ 0.0016, -0.0208, -0.0117,  ...,  0.0706,  0.1028, -0.0254],\n",
       "        ...,\n",
       "        [-0.0088,  0.0209,  0.0469,  ..., -0.0302, -0.0400,  0.0570],\n",
       "        [-0.0276, -0.0037,  0.0601,  ...,  0.0496, -0.0669,  0.0181],\n",
       "        [ 0.0279, -0.0256, -0.0178,  ..., -0.0262,  0.0239, -0.0159]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorec.decoder.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
