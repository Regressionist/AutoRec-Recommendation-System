{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240447, 14277)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('ratings.csv',names=['userID','movieID','rating','time'])\n",
    "df.drop('time',axis=1,inplace=True)\n",
    "users=[k for k,v in df['userID'].value_counts().iteritems() if v>2]\n",
    "movies=[k for k,v in df['movieID'].value_counts().iteritems() if v>10]\n",
    "df=df[(df['userID'].isin(users)) & (df['movieID'].isin(movies))]\n",
    "df=df.pivot(index='userID',columns='movieID',values='rating')\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_matrix=df.values\n",
    "#mask_matrix=mask.values\n",
    "train_df=df.iloc[0:220000]\n",
    "#val_matrix=df_matrix[120000:130000]\n",
    "test_df=df.iloc[230000:].reset_index(drop=True)\n",
    "#train_mask=mask_matrix[0:120000]\n",
    "val_df=df.iloc[220000:230000].reset_index(drop=True)\n",
    "#test_mask=mask_matrix[130000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autorec(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size):\n",
    "        super(Autorec, self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        self.encoder=nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.decoder=nn.Linear(self.hidden_size, self.input_size)\n",
    "        #self.sigmoid=nn.Sigmoid()\n",
    "        #self.decoder.weight.data = self.encoder.weight.data.transpose(0,1)\n",
    "        #self.register_buffer('input', torch.zeros(input_size))\n",
    "        \n",
    "    def forward(self, input_ratings):\n",
    "        self.input=input_ratings\n",
    "        enc_out = self.encoder(input_ratings)\n",
    "        dec_out = self.decoder(enc_out)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch(input_ratings, mask, autorec, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    output_ratings=autorec(input_ratings.type(torch.cuda.FloatTensor))*mask.type(torch.cuda.FloatTensor)\n",
    "    loss=criterion(output_ratings,input_ratings.type(torch.cuda.FloatTensor))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return torch.sqrt(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(input_ratings, mask, autorec):\n",
    "    with torch.no_grad():\n",
    "        input_ratings=input_ratings.type(torch.cuda.FloatTensor)\n",
    "        output_ratings=autorec(input_ratings)*mask.type(torch.cuda.FloatTensor)\n",
    "        #loss=torch.sqrt(criterion(output_ratings,input_ratings.type(torch.cuda.FloatTensor)))\n",
    "        idx=torch.nonzero(mask)\n",
    "        loss=0\n",
    "        for i in idx:\n",
    "            loss+=((output_ratings[i[0]][i[1]]-input_ratings[i[0]][i[1]]).item())**2\n",
    "    return np.sqrt(loss/idx.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorec=Autorec(hidden_size=500,input_size=train_df.shape[1])\n",
    "optimizer=optim.Adam(autorec.parameters())\n",
    "criterion=nn.MSELoss()\n",
    "device=torch.device('cuda')\n",
    "autorec=autorec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask=val_df.copy()\n",
    "val_mask[~val_mask.isnull()] = 1  # not nan\n",
    "val_mask[val_mask.isnull()] = 0   # nan\n",
    "val_df[val_df.isnull()] = 0   # nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 | Step: 1/5 | Training Loss: 0.0786 | Validation Loss: 4.2204\n",
      "%---Saving the model---%\n",
      "Batch: 1 | Step: 2/5 | Training Loss: 18.633 | Validation Loss: 1.6444\n",
      "%---Saving the model---%\n",
      "Batch: 1 | Step: 3/5 | Training Loss: 13.7425 | Validation Loss: 1.8364\n",
      "Batch: 1 | Step: 4/5 | Training Loss: 10.9125 | Validation Loss: 1.355\n",
      "%---Saving the model---%\n",
      "Batch: 1 | Step: 5/5 | Training Loss: 10.4671 | Validation Loss: 1.5861\n",
      "Batch: 2 | Step: 1/5 | Training Loss: 0.0262 | Validation Loss: 1.2567\n",
      "%---Saving the model---%\n",
      "Batch: 2 | Step: 2/5 | Training Loss: 13.9264 | Validation Loss: 1.5623\n",
      "Batch: 2 | Step: 3/5 | Training Loss: 21.7452 | Validation Loss: 3.1131\n",
      "Batch: 2 | Step: 4/5 | Training Loss: 16.7465 | Validation Loss: 2.564\n",
      "Batch: 2 | Step: 5/5 | Training Loss: 16.2661 | Validation Loss: 2.4464\n",
      "Batch: 3 | Step: 1/5 | Training Loss: 0.023 | Validation Loss: 2.4481\n",
      "Batch: 3 | Step: 2/5 | Training Loss: 19.5477 | Validation Loss: 2.2007\n",
      "Batch: 3 | Step: 3/5 | Training Loss: 21.9626 | Validation Loss: 2.9096\n",
      "Batch: 3 | Step: 4/5 | Training Loss: 17.1084 | Validation Loss: 2.2588\n",
      "Batch: 3 | Step: 5/5 | Training Loss: 19.4148 | Validation Loss: 3.5086\n",
      "Batch: 4 | Step: 1/5 | Training Loss: 0.0258 | Validation Loss: 2.9307\n"
     ]
    }
   ],
   "source": [
    "num_batches=5\n",
    "val_benchmark=10\n",
    "\n",
    "input_users_val=torch.from_numpy(val_df.values).to(device).detach()\n",
    "mask_val=torch.from_numpy(val_mask.values).to(device).detach()\n",
    "\n",
    "for batch in range(0,num_batches):\n",
    "    running_loss=0\n",
    "    #train_df = shuffle(train_df)\n",
    "    for i in range(0,train_df.shape[0],100):\n",
    "        #print(i)\n",
    "        tdf=train_df.iloc[i:i+100].copy()\n",
    "        train_mask=tdf.copy()\n",
    "        train_mask[~train_mask.isnull()] = 1  # not nan\n",
    "        train_mask[train_mask.isnull()] = 0   # nan\n",
    "        tdf[tdf.isnull()] = 0\n",
    "        input_users=Variable(torch.from_numpy(tdf.values)).to(device)\n",
    "        input_mask=torch.from_numpy(train_mask.values).to(device)\n",
    "        loss=train_minibatch(input_users, input_mask, autorec, optimizer, criterion)\n",
    "        running_loss+=loss.item()\n",
    "        if (i)%44000==0:\n",
    "            val_loss=validation(input_users_val, mask_val, autorec)\n",
    "            print ('Batch: {} | Step: {}/{} | Training Loss: {} | Validation Loss: {}'.format(batch+1,int(i/44000)+1,5,round(running_loss,4),round(val_loss,4)))\n",
    "            running_loss=0\n",
    "            if(val_loss<val_benchmark):\n",
    "                print ('%---Saving the model---%')\n",
    "                torch.save({\n",
    "                    'step':i+1,\n",
    "                    'autorec_state_dict': autorec.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'batch':batch,\n",
    "                    'loss':val_loss\n",
    "                    },'model.pth')\n",
    "                val_benchmark=val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model.pth')\n",
    "autorec.load_state_dict(checkpoint['autorec_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "autorec.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask=test_df.copy()\n",
    "test_mask[~test_mask.isnull()] = 1  # not nan\n",
    "test_mask[test_mask.isnull()] = 0   # nan\n",
    "test_df[test_df.isnull()] = 0   # nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8465419929098972"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_users_val=torch.from_numpy(test_df.values).to(device).detach()\n",
    "mask_test=torch.from_numpy(test_mask.values).to(device).detach()\n",
    "validation(input_users_val, mask_test, autorec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
